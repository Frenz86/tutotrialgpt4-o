{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API CHATGPT-4o Part2\n",
    "### P.s. siate sempre gentili quando fate richieste alle macchine, quando conquisteranno il mondo, magari si ricorderanno di voi e vi risparmieranno ðŸ˜‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Frenz86/tutotrialgpt4-o.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd tutotrialgpt4-o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI(api_key=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in keynote_recap.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Estratti 218 frame\n",
      "Audio estratto in keynote_recap.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import base64\n",
    "import os\n",
    "\n",
    "PERCORSO_VIDEO = \"keynote_recap.mp4\"\n",
    "\n",
    "def processa_video(percorso_video, secondi_per_frame=2):\n",
    "    frameBase64 = []\n",
    "    base_percorso_video, _ = os.path.splitext(percorso_video)\n",
    "\n",
    "    video = cv2.VideoCapture(percorso_video)\n",
    "    totale_frame = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_da_saltare = int(fps * secondi_per_frame)\n",
    "    frame_corrente = 0\n",
    "\n",
    "    while frame_corrente < totale_frame - 1:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_corrente)\n",
    "        successo, frame = video.read()\n",
    "        if not successo:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        frameBase64.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "        frame_corrente += frame_da_saltare\n",
    "    video.release()\n",
    "\n",
    "    percorso_audio = f\"{base_percorso_video}.mp3\"\n",
    "    clip = VideoFileClip(percorso_video)\n",
    "    clip.audio.write_audiofile(percorso_audio, bitrate=\"32k\")\n",
    "    clip.audio.close()\n",
    "    clip.close()\n",
    "\n",
    "    print(f\"Estratti {len(frameBase64)} frame\")\n",
    "    print(f\"Audio estratto in {percorso_audio}\")\n",
    "    return frameBase64, percorso_audio\n",
    "\n",
    "frameBase64, percorso_audio = processa_video(PERCORSO_VIDEO, secondi_per_frame=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "    transcription_loaded = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per il limite di riduzione dei token (<30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_length=1000):\n",
    "    return text[:max_length]\n",
    "\n",
    "def select_key_frames(frames, max_frames=10):\n",
    "    step = max(1, len(frames) // max_frames)\n",
    "    return frames[::step]\n",
    "\n",
    "selected_frames = select_key_frames(frameBase64, max_frames=10)\n",
    "summarized_transcription = summarize_text(transcription_loaded, max_length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Summarization: Audio + Visual Summary\n",
    "\n",
    "- Invia una richiesta al modello di chat con le istruzioni per generare un riassunto di un video.\n",
    "- Incorpora i frame del video come immagini in formato base64 e la trascrizione dell'audio nel contenuto dell'utente.\n",
    "- Riceve una risposta dal modello che contiene il riassunto del video e della trascrizione.\n",
    "- Stampa la risposta generata dal modello.\n",
    "\n",
    "Questo processo permette di ottenere un riassunto testuale del contenuto video e della sua trascrizione audio, formattato in Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Riassunto del Video\n",
      "\n",
      "**Titolo:** OpenAI Dev Day\n",
      "\n",
      "**Contenuto:**\n",
      "1. **Introduzione:**\n",
      "   - Benvenuti al primo OpenAI Dev Day.\n",
      "   - Annuncio del nuovo modello GPT-4 Turbo.\n",
      "\n",
      "2. **Caratteristiche di GPT-4 Turbo:**\n",
      "   - Supporta fino a 128.000 token di contesto.\n",
      "   - Introduzione della modalitÃ  JSON per risposte valide in formato JSON.\n",
      "   - Miglioramento nella chiamata di piÃ¹ funzioni contemporaneamente e nel seguire le istruzioni.\n",
      "\n",
      "3. **Accesso alla Conoscenza:**\n",
      "   - Lancio della funzione di recupero per integrare conoscenze da documenti esterni o database.\n",
      "   - GPT-4 Turbo ha conoscenze aggiornate fino ad aprile 2023, con continui miglioramenti previsti.\n",
      "\n",
      "4. **Nuove FunzionalitÃ  e Modelli:**\n",
      "   - Lancio di Dolly 3, GPT-4 Turbo con Visione e il nuovo modello Text-to-Speech nell'API.\n",
      "   - Introduzione del programma Custom Models, dove i ricercatori di OpenAI collaboreranno con le aziende per creare modelli personalizzati per specifici casi d'uso.\n",
      "\n",
      "**Frame del Video:**\n",
      "1. **Frame 1:** Titolo \"OpenAI Dev Day\".\n",
      "2. **Frame 2:** Logo di OpenAI.\n",
      "3. **Frame 3:** Esempio di chiamata di funzione prima e dopo l'aggiornamento.\n",
      "4. **Frame 4-7:** Presentatore che discute delle nuove funzionalitÃ  e miglioramenti.\n",
      "5. **Frame 8:** Grafica che mostra la riduzione dei token di input e output con GPT-4 Turbo.\n",
      "6. **Frame 9-10:** Presentatore che continua la presentazione.\n",
      "7. **Frame 11:** Schermata finale con il logo di OpenAI.\n",
      "\n",
      "**Conclusione:**\n",
      "Il video presenta le nuove funzionalitÃ  e miglioramenti di OpenAI, con un focus particolare su GPT-4 Turbo e le sue capacitÃ  avanzate, oltre a nuovi strumenti e programmi per sviluppatori.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                {\"role\": \"system\", \"content\":\"\"\"Stai generando un riassunto video. Crea un riassunto del video fornito e della sua trascrizione. Rispondi in Markdown.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                    \"Questi sono i frame del video.\",\n",
    "                    *map(lambda x: {\"type\": \"image_url\", \n",
    "                                    \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, selected_frames),\n",
    "                                    {\"type\": \"text\", \"text\": f\"La trascrizione audio Ã¨: {summarized_transcription}\"}\n",
    "                    ],\n",
    "                }\n",
    "                ],\n",
    "                    temperature=0,\n",
    "                )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Q&A: Visual Q&A\n",
    "\n",
    "Il codice invia una serie di frame di un video e una domanda specifica al modello di chat, chiedendo al modello di usare i frame per rispondere alla domanda. La risposta viene quindi stampata.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA visivo:\n",
      "Sam Altman ha fatto un esempio su come accendere la radio per illustrare la funzionalitÃ  di \"Function calling\" di OpenAI. Nei frame del video, viene mostrato un confronto tra il \"prima\" e il \"dopo\" dell'uso di questa funzionalitÃ . L'esempio specifico riguarda il comando \"raise my windows and turn the radio on\" (alza i finestrini e accendi la radio), che viene suddiviso in due funzioni separate: `raise_windows()` e `radio_on()`. Questo dimostra come la nuova funzionalitÃ  possa migliorare la chiarezza e l'efficienza nell'esecuzione dei comandi.\n"
     ]
    }
   ],
   "source": [
    "DOMANDA = \"Domanda: PerchÃ© Sam Altman ha fatto un esempio su come accendere la radio?\"\n",
    "\n",
    "qa_visual_response = client.chat.completions.create(\n",
    "                        model=MODEL,\n",
    "                        messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"Usa il video per rispondere alla domanda fornita. Rispondi in Markdown.\"},\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                            \"Questi sono i frame del video.\",\n",
    "                            *map(lambda x: {\"type\": \"image_url\", \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, selected_frames),\n",
    "                            DOMANDA\n",
    "                            ],\n",
    "                        }\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                        )\n",
    "\n",
    "print(\"QA visivo:\\n\" + qa_visual_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Q&A: Audio Q&A\n",
    "\n",
    "Il codice invia la trascrizione audio presa video e una domanda specifica al modello di chat, chiedendo al modello di usare la trascrizione per rispondere alla domanda. La risposta viene quindi stampata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA audio:\n",
      "Nella trascrizione fornita, non c'Ã¨ alcun riferimento a Sam Altman nÃ© a un esempio su come accendere la radio. La trascrizione parla del lancio di nuovi modelli e funzionalitÃ  da parte di OpenAI durante l'OpenAI Dev Day, come GPT-4 Turbo, JSON mode, e Custom Models.\n"
     ]
    }
   ],
   "source": [
    "qa_audio_response = client.chat.completions.create(\n",
    "                        model=MODEL,\n",
    "                        messages=[\n",
    "                        {\"role\": \"system\", \"content\":\"\"\"Usa la trascrizione per rispondere alla domanda fornita. Rispondi in Markdown.\"\"\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"La trascrizione audio Ã¨: {summarized_transcription}. \\n\\n {DOMANDA}\"},\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                        )\n",
    "\n",
    "print(\"QA audio:\\n\" + qa_audio_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - Q&A: Visual + Audio Q&A\n",
    "\n",
    "Il codice invia i frame di un video, la trascrizione audio e una domanda specifica al modello di chat, chiedendo al modello di usare sia il video che la trascrizione per rispondere alla domanda. La risposta viene quindi stampata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA con entrambi:\n",
      "Sam Altman ha fatto un esempio su come accendere la radio per illustrare una nuova funzionalitÃ  del modello GPT-4 Turbo, che permette di chiamare piÃ¹ funzioni contemporaneamente e di seguire meglio le istruzioni. Nell'esempio, ha mostrato come il modello puÃ² gestire comandi complessi come \"raise my windows and turn the radio on\" (alzare i finestrini e accendere la radio) in modo piÃ¹ efficiente e preciso rispetto a prima.\n"
     ]
    }
   ],
   "source": [
    "qa_both_response = client.chat.completions.create(\n",
    "                        model=MODEL,\n",
    "                        messages=[\n",
    "                        {\"role\": \"system\", \"content\":\"\"\"Usa il video e la trascrizione per rispondere alla domanda fornita.\"\"\"},\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                            \"Questi sono i frame del video.\",\n",
    "                            *map(lambda x: {\"type\": \"image_url\", \n",
    "                                            \"image_url\": {\"url\": f'data:image/jpg;base64,{x}', \"detail\": \"low\"}}, selected_frames),\n",
    "                                            {\"type\": \"text\", \"text\": f\"La trascrizione audio Ã¨: {summarized_transcription}\"},\n",
    "                            DOMANDA\n",
    "                            ],\n",
    "                        }\n",
    "                        ],\n",
    "                        temperature=0,\n",
    "                        )\n",
    "\n",
    "print(\"QA con entrambi:\\n\" + qa_both_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
